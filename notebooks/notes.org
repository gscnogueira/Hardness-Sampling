#+title: Análise dos resultados

- Questionamento importante :: 100 queries faz realmente sentido? O que de fato eu estou avaliando?
- Estou avaliando qual método é melhor em em AL em 100 queries de um determinado dataset D.
- Muitos datasets podem saturar antes das 100 queries e outros podem nem fazer cosquinha. Mas isso indica alguma tendencia?
 Acho que minha maior insegurança é: em datasets muito grande, será que 100 queries é suficiente para que o AL faça diferença, ou será que os modelos estão meio zoados ainda? Limitação que é possível elaborar

* TODO AULC média

Os resultados mostram que não podemos afirmar que o método que obteve o melhor rank médio foi melhor que a amostragem aleatória (Rnd). Dessa forma, não podemos temos evidência o suficiente para comprovar o beneficio do AL nesse conjunto de dadasets. Entretanto, é importante destacar que a afirmação anterior é limitada as técnicas de amostragem utilizadas. É possível que exista beneficio nesse conjunto de dados ao utilizar outras técnicas. 

Além disso, os resultados do Davi mostram que existe diferença significativa em aplicar o AL. Dessa forma, é importante consultar esses resultados novamente, para verificar se existe contradição, ou se foram utilizados métodos que apresentam uma significancia maior.

Os testes mostram que:
1. Não podemos dizer que algum método é melhor que Rnd

2. Para o conjunto D, podemos dizer com com uma cofiança de 99% que:
   1. _Rnd é melhor que EER e ID_.
   2. _Rnd é melhor que MS para CART e NB_.
   3. _Rnd é melhor que TU para 5NN e SVM_.
   4. Apesar de sua colocação superior, _não temos confiança suficiente_ para dizer que _TU é melhor que Rnd para NB e CART_

3. No que diz respeito ao HardS
   1. No geral, Rnd é melhor que F1, F2 e F3 com (p=0.01)
   2. No geral, Rnd, é melhor que medidas Feature-based (p=0.05)
   3. No geral, Rnd é melhor que medidas Class Balance (p=0.05)
   4. Apesar do desempenho de Rnd, não temos confiança para afirmar que ele é melhor que _kDN, LSR e LSCI_ em nenhum dos casos, são melhores recomendações gerais
   5. Fica menos evidente quais seriam as escolhas adequadas para 5NN e SVM, em ambos os casos varias medidas não podem ser estatitiscamente diferenciadas de Rnd e varias outras.
   6. Fica evidente que, tanto as medidas Class Balance, quanto as Medidas Feature based não funcionam como boas medidas de infomatividade para o aprendizado ativo.
   7. Surepreeendentemente, N2I, aprsentou um desempenho muito abaixo das outras medidas de seu grupo, talvez isso deva ser investigado

** Comparação com o topline

#+CAPTION:Medidas que não foram piores que o topline.
| HM   | Grupo   | CART (TU) | NB (TU) | 5NN (Rnd) | SVM (kDN) |
|------+---------+-----------+---------+-----------+-----------|
| LSR  | Neigh   | *         | *       | *         | *         |
|------+---------+-----------+---------+-----------+-----------|
| kDN  | Neigh   |           |         | *         | *         |
| LSCI | Neigh   |           |         | *         | *         |
| CLD  | Likeli. |           |         | *         | *         |
| TDu  | Tree    |           |         | *         | *         |
| DCP  | Tree    |           |         | *         | *         |
| DS   | Tree    |           |         | *         | *         |
|------+---------+-----------+---------+-----------+-----------|
| N1I  | Neigh   |           |         |           | *         |
| CL   | Likeli. |           |         |           | *         |

Tabela mostra que:
1. Não podemos dizer que qualquer topline foi melhor que o LSR
2. Para 5NN e NB fica mais dificil dizer quem foi o melhor de fato.

** Clássicos X Baseline

#+caption: Métodos classicos que foram batidos pelo baseline
| Método | 5NN | CART | NB | SVM |
|--------+-----+------+----+-----|
| EER    | *   | *    | *  | *   |
| ID     | *   | *    | *  | *   |
| MS     |     | *    | *  |     |
| TU     | *   |      |    | *   |

Com base nos resultados podemos, podemos afirmar com 99% de certeza que a amostragem aleatória foi melhor que EER e ID, independente do learner.

Além disso, para CART e NB e amostragem aleatória se mostrou melhor do que MS (o que pode ser um indicativo de que ela não seja tão boa assim quando outros classificadores são utilizados)

** HMs vs Baseline

#+CAPTION:Métodos HardS batidos pelo baseline
| Método | 5NN | CART | NB | SVM |
|--------+-----+------+----+-----|
| kDN    |     |      |    |     |
| LSR    |     |      |    |     |
| LSCI   |     |      |    |     |
| TDu    |     | *    |    |     |
| CLD    |     | +    | .  |     |
| DCP    |     | *    | .  |     |
| TDp    |     | *    | *  |     |
| DS     |     | *    | *  |     |
|--------+-----+------+----+-----|
| N1I    | +   | *    | +  |     |
| H      | +   | +    | *  |     |
| CL     | *   | +    | *  |     |
|--------+-----+------+----+-----|
| F4I    | *   | +    | *  | *   |
| MV     | *   | *    | *  | +   |
| CB     | *   | *    | *  | +   |
| N2I    | *   | *    | *  | *   |
| F3I    | *   | *    | *  | *   |
| F1I    | *   | *    | *  | *   |
| F2I    | *   | *    | *  | *   |

- Descartar Feature-based
- Descartar CB
- Fora o SVM, podemos descartar N1I, H, e CL
- Não temos certeza se Rnd é melhor do que:
  - 5NN :: kDN, LSR, LSCI, TDu, CLD, DCP, TDp, DS
  - CART :: kDN, LSR, LSCI
  - NB :: kDN, LSR, LSCI, TDu,
  - SVM :: kDN, LSR, LSCI, TDu, CLD, DCP, TDp, DS, N1I, H, CL
** Comparação com métodos clássicos
*** 5NN
1. Qualquer alternativa é melhor que ID
2. TDu e MS estão *bem próximos* em rank médio.
3. NINGUÉM é MELHOR que o Rnd
4. Mta gnt é PIOR que o Rnd
   1. Incluindo 3/4 dos clássicos
5. Baseline, Clássico e HMs TDu, MS, kDN, LSR, CLD, DCP, TDp, LSCI e DS competem pela primeira posição
   * Clássico :: MS
   * HMs :: TDu, kDN, LSR, CLD, DCP, TDp, LSCI e DS
6. Desses, conseguiram superar os classicos:
   * TU  :: Rnd, _TDu_, MS, _kDN_, _LSR_, _CLD_
   * EER :: Rnd, TDu, MS, kDN, LSR, CLD, DCP, TDP
   * ID  :: Rnd, TDu, MS, kDN, LSR, CLD, DCP, TDP, LSCI, DS
*** CART
1. Muitas medidas parecem ter ido igualmente mal, abaixo de N2I
2. Nesse gripo estão as MS, EER e ID (sendo essa melhor colocada que as duas ultimas)
3. Apenas, TU, Rnd, e LSR competem pela primeira posição
4. LSR é pior que TU com p=0.1
5. O segundo lugar pode ser disputado entre: Rnd, LSR, LSCI e kDN. Esses ultimos dois definitivamente não estão em primeiro.
6. Todos esses apresentaram uma colocação melhor que os métodos clássicos com p=0.01.
*** NB 
1. Aqui, ID pareceu ir um pocuo melhor, mas mesmo assim não foi mto bem
2. F3I, MV, CB, F1I foram desbancadas por quase todo mundo
3. TU, Rnd e LSR ficaram concorrendo pela primeira posição. Os 3 melhores que qualquer método clássico
4. Embora, Rnd, LSR, TDu, kDN, LSCI, DCP e CLD ficaram em disputa pela segunda, apenas Rnd e LSR foram efetivamente melhor que os outros classicos. TDu e kDN podem ser tidos como melhores que EER e ID e o resto apenas ID, o que não significa mta coisa.
*** SVM
1. Competição mais acirrada pelo primeiro lugar:
   - _kDN_, _LSR_, _CLD_, Rnd, _TDu_, _LSCI_, _DCP_, _DS_, _N1I_, _CL_, MS
2. Proximidade muito maior de rank a partir de CLD e MS
3. Todos esses que disputam o primeiro são efetivamente melhores que os métodos clássicos

* TODO Curvas de Ranking

Responder as perguntas:
1. Algum método do HardS é superior às estratégias clássicas durante o processo de AL?
2. Algum método do HardS é superior às estratégias clássicas em algum momento do processo de AL

* Questões Pendentes
- [ ] Analisar resultados do Davi.
- [ ] Verificar se existe uma diferença do resultado macro quando dividimos os conjuntos de dados em grupos menores (por exemplo, agrupar por tamanho do dataset).
- [ ] Listar limitações das analises e das suposições feitas (e.g. por quê 100 consultas? O que de fato estamos comparando).
- [ ] N2I deveria ter ido tão mal assim? Ela é bem parecida com LSR e LSCI

